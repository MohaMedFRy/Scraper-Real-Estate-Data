{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debe9500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import link\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "import datetime\n",
    "from pandas.core.common import random_state\n",
    "from unidecode import unidecode\n",
    "\n",
    "from itertools import zip_longest\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "import pprint \n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7d4e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-c01dcdeedd39>:99: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['price'] = df['price'].str.replace('ج.م', '').str.replace(',', '')\n"
     ]
    }
   ],
   "source": [
    "def olx_scraping():\n",
    "    def all_link(url):\n",
    "      pNumber = 1\n",
    "      all=[]\n",
    "      while True:\n",
    "        headers =  {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',}\n",
    "        page = requests.get(url+ \"?page=\" + str(pNumber) , headers=headers)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        all_links = soup.find_all('a', class_='ads__item__ad--title')\n",
    "        \n",
    "        for i in all_links:\n",
    "            all.append(i.attrs['href'])\n",
    "        if soup.find(\"span\",{\"class\":\"item fright\"}) ==None:\n",
    "          break\n",
    "        else:\n",
    "            pNumber+=1\n",
    "      return all\n",
    "    list_of_dictt = []  \n",
    "    def appartment_data(url,location,Type,r_s):\n",
    "      headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',}\n",
    "      list_of_links = all_link(url)\n",
    "      for link in list_of_links:\n",
    "        appartment_dict ={}\n",
    "        result = requests.get(link,headers=headers)\n",
    "        soup = BeautifulSoup(result.content,\"lxml\")\n",
    "        table = soup.find_all(\"table\",class_=\"item\")\n",
    "        for i in table:\n",
    "          th = i.find('th').text.replace(\"\\t\",\"\").replace(\"\\n\",\" \").strip()\n",
    "          strong = i.find('strong').text.replace(\"\\t\",\"\").replace(\"\\n\",\" \").strip()\n",
    "          appartment_dict[th] = strong\n",
    "        try:\n",
    "          row = soup.find('ul' , {'class':'clearfix'}).find_all('li')\n",
    "          appartment_dict['unit'] = row[2].text.replace(\"\\t\",\"\").replace(\"\\n\",\" \")\n",
    "        except:\n",
    "          appartment_dict['unit'] =None\n",
    "        try:\n",
    "          appartment_dict[\"price\"]=soup.find(\"div\",id=\"offerbox\").find(\"strong\").text.replace(\"\\t\",\" \").replace(\"\\r\",\" \").replace(\"\\n\",\" \")\n",
    "        except:\n",
    "          appartment_dict[\"price\"]=None\n",
    "        try:\n",
    "          appartment_dict[\"description\"]=soup.find(\"p\",{\"class\":\"pding10 lheight20 large\"}).text.replace(\"\\t\",\"\").replace(\"\\r\",\" \").replace(\"\\n\",\"\")\n",
    "        except:\n",
    "          appartment_dict[\"description\"]=None\n",
    "          \n",
    "        appartment_dict[\"city\"]=location\n",
    "        try:\n",
    "          latitude = soup.findAll(\"div\", class_=\"bgfff hidden br-1 vtop mapcontainer\")\n",
    "          data = [t.get('data-lat') for t in latitude if t.get('data-lat')] \n",
    "          appartment_dict[\"lat\"]=None\n",
    "        except:\n",
    "          appartment_dict[\"lat\"]=None\n",
    "        try:\n",
    "          longitude = soup.findAll(\"div\", class_=\"bgfff hidden br-1 vtop mapcontainer\")\n",
    "          data = [t.get('data-lon') for t in longitude if t.get('data-lon')] \n",
    "          appartment_dict[\"lon\"]=None\n",
    "        except:\n",
    "          appartment_dict[\"lon\"]=None\n",
    "        try:   \n",
    "          appartment_dict[\"location1\"]=soup.find(\"strong\",{\"class\":\"c2b small\"}).string.replace(\"\\t\",\" \").replace(\"\\n\",\" \")\n",
    "        except:\n",
    "          appartment_dict[\"location1\"]=None\n",
    "        try:   \n",
    "          appartment_dict[\"location2 \"]=appartment_dict[\"location1\"]\n",
    "        except:\n",
    "          appartment_dict[\"location2\"]=None\n",
    "        appartment_dict[\"type\"]= Type\n",
    "        appartment_dict[\"rent_sale\"]=r_s\n",
    "        try:\n",
    "          appartment_dict[\"web_name\"]=soup.find(\"a\",id=\"headerLogo\").text.strip()\n",
    "        except:\n",
    "          appartment_dict[\"web_name\"]=None\n",
    "        appartment_dict[\"link\"]= link\n",
    "        list_of_dictt.append(appartment_dict)\n",
    "      return (list_of_dictt) \n",
    "    location = 'cairo'\n",
    "    types=['apartments-duplex-for','villas-for','commercial-for','buildings-lands-other']\n",
    "    for Type in types:\n",
    "        if Type=='buildings-lands-other':\n",
    "            rent_or_sale = ['1','2']\n",
    "            for r_s in rent_or_sale:\n",
    "                url = f\"https://www.olx.com.eg/properties/{Type}/{location}/?search%5Bfilter_enum_sale_rent%5D%5B0%5D={r_s}\"\n",
    "                appartment_data(url,location,Type,r_s )\n",
    "        else:\n",
    "            rent_sale = ['sale','rent']\n",
    "            for r_s in rent_sale:\n",
    "                url = f\"https://www.olx.com.eg/properties/{Type}-{r_s}/{location}/\"\n",
    "                appartment_data(url,location,Type,r_s ) \n",
    "  \n",
    "    \n",
    "\n",
    "  # #clean of data\n",
    " \n",
    "    df = pd.DataFrame.from_dict(list_of_dictt)\n",
    "    # df.to_csv('olx_Allcairo_scraped.csv', encoding='utf-8-sig', index=False)\n",
    "    df.drop(columns=['الكماليات','الطابق','مقدم','كمبوند','مفروش','unit','شروط التسليم','تاريخ التسليم','طريقة الدفع'], inplace=True)\n",
    "\n",
    "    df=df.rename(columns={'غرف نوم':'bedrooms','الحمامات':'bathrooms','المساحة (م٢)':'size','النوع':'unit'})\n",
    "\n",
    "    df['price'] = df['price'].str.replace('ج.م', '').str.replace(',', '')\n",
    "    # # change bathrooms arabic numbers\n",
    "    df['bedrooms']=df['bedrooms'].replace('١٠+','10')\n",
    "    df['bathrooms']=df['bathrooms'].replace('١٠+','10')\n",
    "    df[\"bathrooms\"] = df[\"bathrooms\"].fillna('0').astype(int)\n",
    "    df[\"bedrooms\"] = df[\"bedrooms\"].fillna('0').astype(int)\n",
    "    df[\"bedrooms\"] = df[\"bedrooms\"].replace(0,np.nan)\n",
    "    df[\"bathrooms\"] = df[\"bathrooms\"].replace(0,np.nan)\n",
    "\n",
    "    df['size'] = pd.to_numeric(df['size'], errors='coerce')\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    df = df.dropna(subset=['unit'])\n",
    "\n",
    "    df['view'] = None\n",
    "    df['source']='scraped'\n",
    "    df['price_per_unit']=df['price']/df['size']\n",
    "    df['price_per_unit']=df['price_per_unit'].round(2)\n",
    "    df = df[df['location1'].str.contains('القاهرة')]\n",
    "    column_names = ['link',\n",
    "                    'bathrooms',\n",
    "                    'bedrooms',\n",
    "                    'city',\n",
    "                    'location1',\n",
    "                    'location2',\n",
    "                    'price',\n",
    "                    'size',\n",
    "                    'price_per_unit',\n",
    "                    'type',\n",
    "                    'unit',\n",
    "                    'lat',\n",
    "                    'lon',\n",
    "                    'view',\n",
    "                    'description',\n",
    "                    'web_name',\n",
    "                    'source',\n",
    "                    'rent_sale']\n",
    "    df = df.reindex(columns=column_names)\n",
    "\n",
    "    df['type']=df['type'].replace('apartments-duplex-for','سكنى').replace('villas-for','سكنى').replace('commercial-for','تجارى').replace('buildings-lands-other','أراضي')\n",
    "    df['unit']=df['unit'].replace('فيلا منفصلة','فلل')\n",
    "    df.loc[df['unit'] == 'محل', 'type'] = 'تجارى'\n",
    "    df.loc[df['unit'] == 'مكتب', 'type'] = 'إدارى'\n",
    "    df.loc[df['unit'] == 'مطعم و كافيه', 'type'] = 'تجارى'\n",
    "    df.loc[df['unit'] == 'مصنع', 'type'] = 'تجارى'\n",
    "    df.loc[df['unit'] == 'مخزن', 'type'] = 'تجارى'\n",
    "    df.loc[df['unit'] == 'جراج', 'type'] = 'تجارى'\n",
    "    df.loc[df['unit'] == 'عيادة', 'type'] = 'إدارى'\n",
    "    df.loc[df['unit'] == 'مبنى تجارى كامل', 'type'] = 'تجارى'\n",
    "    df['unit']=df['unit'].replace('سكنية','أرض مباني').replace('لكل الأغراض','أرض مباني').replace('زراعية','أرض زراعية').replace('تجارية','أرض تجارية')\n",
    "    df.drop(df[df['unit'] == 'أخرى'].index, inplace = True)\n",
    "    df['rent_sale']=df['rent_sale'].replace('rent','إيجار').replace('sale','بيع').replace('2','إيجار').replace('1','بيع')\n",
    "    df['city']=df['city'].replace('cairo','القاهرة')\n",
    "    df['web_name']=df['web_name'].replace('OLX Egypt','أوليكس')\n",
    "\n",
    "    df['location1']=df['location1'].str.split('،').str[0]\n",
    "    df['location1']=df['location1'].str.split(',').str[0]\n",
    "    df['location2']=df['location1']\n",
    "\n",
    "    df = df.dropna( how='any', subset=['price'])\n",
    "    df = df.dropna( how='any', subset=['size'])\n",
    "    df = df.dropna( how='any', subset=['location1'])\n",
    "    df.to_csv('olx_Allcairo_clean.csv' ,index=False, encoding ='utf-8-sig')\n",
    "olx_scraping()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798a7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  2022-01-19 22:08:39.949179\n",
      "100\n",
      "end  2022-01-19 22:10:05.113749\n",
      "duration  0:01:25.164570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-d34e7c14052f>:151: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['cleaned_bathrooms'] = df['bathrooms'].str.replace(\"+\", \"\")\n"
     ]
    }
   ],
   "source": [
    "def propertyfinder_Scraping():    \n",
    "    start = datetime.datetime.now()\n",
    "    print(\"start \",start)\n",
    "    \n",
    "    location = []\n",
    "    view = []\n",
    "    rent = \"ايجار\"\n",
    "    sale = \"بيع\"\n",
    "    commercial = \"تجاري\"\n",
    "    house = \"سكني\"\n",
    "    Country = \"مصر\"\n",
    "    category_Dict = {1: [sale, house] ,2: [rent, house],\n",
    "                     3: [sale, commercial], 4: [rent, commercial]}\n",
    "    dict_total = {}\n",
    "    list_dict = []\n",
    "    #category_num =2\n",
    "    basic_url = \"https://www.propertyfinder.eg\"\n",
    "    \n",
    "    counter = 0\n",
    "    links = []\n",
    "    for category_num in (category_Dict.keys()):\n",
    "        \n",
    "        page_num = 0\n",
    "        while (True):\n",
    "            page_num += 1\n",
    "            result = requests.get(\n",
    "                f\"https://www.propertyfinder.eg/ar/search?c={category_num}&dp=0&l=2254&ob=mr&page={page_num}\")\n",
    "            # save page content\n",
    "            src = result.content\n",
    "            # create \"soup\" object to parse content\n",
    "            soup = BeautifulSoup(src, \"lxml\")\n",
    "            # retreive html source code of the block\n",
    "            ad_blocks = soup.find_all(\"div\", {\"class\": \"card-list__item\"})\n",
    "    \n",
    "            # get the hyperlink carrying the whole information of the block (webpage)\n",
    "            for i in range(len(ad_blocks)):\n",
    "                l = basic_url+ad_blocks[i].find(\"a\").attrs['href']\n",
    "                links.append([l,category_num])\n",
    "            next_page_link = soup.find(\n",
    "                \"a\", {\"class\": \"pagination__link pagination__link--next\"})\n",
    "#             if (page_num == 1 ):\n",
    "            if(next_page_link == None):\n",
    "                # print(page_num)\n",
    "                break\n",
    "    \n",
    "        #json_path = \"/content/drive/MyDrive/Colab Notebooks/All_Cairo_dataRent\"\n",
    "    \n",
    "#     print(len(links))\n",
    "    thousand = 1\n",
    "    for link,cat_num in (links):\n",
    "    \n",
    "            result = requests.get(link)\n",
    "            src = result.content\n",
    "            soup = BeautifulSoup(src, \"lxml\")\n",
    "            dict_total['link'] = link\n",
    "    \n",
    "            grand_parent = soup.find(\"div\", {\"class\": \"property-facts\"})\n",
    "        # Get the unit (نوع العقار)\n",
    "            try:\n",
    "                icon = (grand_parent.find(\n",
    "                    \"svg\", {\"class\": \"property-facts__icon\", \"viewbox\": \"0 0 17 18\"}))\n",
    "    \n",
    "                dict_total['unit'] = (\n",
    "                    icon.parent.next_sibling.text.replace(\"  \", \"\").replace(\"\\n\", \"\"))\n",
    "            except:\n",
    "                dict_total['unit'] = None\n",
    "    \n",
    "        # Get the area of the unit in \"Meters\" and cast it to integer\n",
    "            try:\n",
    "                icon = (grand_parent.find(\n",
    "                    \"svg\", {\"class\": \"property-facts__icon\", \"viewbox\": \"0 0 18 18\"}))\n",
    "                a_string = icon.parent.next_sibling.span.next_sibling.next_sibling.text\n",
    "    \n",
    "                dict_total['size'] = (int(a_string.split()[0].replace(\",\", \"\")))\n",
    "            except:\n",
    "                dict_total['size'] = None\n",
    "    \n",
    "        # Get the number of rooms and cast it to integer\n",
    "            try:\n",
    "                icon = (grand_parent.find(\n",
    "                    \"svg\", {\"class\": \"property-facts__icon\", \"viewbox\": \"0 0 19 15\"}))\n",
    "                dict_total['bedrooms'] = (\n",
    "                    icon.parent.next_sibling.text.replace(\"\\n\", \"\"))\n",
    "    \n",
    "            except:\n",
    "                dict_total['bedrooms'] = None\n",
    "            try:\n",
    "                icon = (grand_parent.find(\n",
    "                    \"svg\", {\"class\": \"property-facts__icon\", \"viewbox\": \"0 0 19 13\"}))\n",
    "                dict_total['bathrooms'] = (\n",
    "                    icon.parent.next_sibling.text.replace(\" \", \"\"))\n",
    "            except:\n",
    "                dict_total['bathrooms'] = None\n",
    "    \n",
    "        # Get the number of bathrooms , price and meter price\n",
    "            try:\n",
    "                dict_total['price'] = int(\n",
    "                    soup.find(\"div\", {\"class\": \"property-price\"}).text.split()[0].replace(\",\", \"\"))\n",
    "                dict_total['price_per_unit'] = (\n",
    "                    dict_total['price'] / dict_total['size'])\n",
    "            except:\n",
    "                dict_total['price'] = None\n",
    "                dict_total['price_per_unit'] = None\n",
    "    \n",
    "        # Get the city,location1 and location2\n",
    "            try:\n",
    "                source = soup.find(\"div\", {\"class\": \"breadcrumb\"})\n",
    "                child = source.find_all(\n",
    "                    \"a\", {\"class\": \"text text--size1 link link--underline\"})\n",
    "                dict_total['city'] = (child[0].text)\n",
    "                dict_total['location1'] = (child[1].text)\n",
    "                if (len(child) > 2):\n",
    "                    dict_total['location2'] = ((child[2].text))\n",
    "    \n",
    "                else:\n",
    "                    dict_total['location2'] = (child[1].text)\n",
    "            except:\n",
    "                dict_total['location1'] = None\n",
    "                dict_total['location2'] = None\n",
    "    \n",
    "            # تفاصيل و مزايا العقار\n",
    "            try:\n",
    "                dict_total['description'] = (soup.find(\n",
    "                    \"div\", {\"class\": \"property-page__description\"}).div.text.replace(\"\\n\", \"\"))\n",
    "            except:\n",
    "                dict_total['description'] = None\n",
    "    \n",
    "            dict_total['web_name'] = \"Property finder\"\n",
    "            dict_total['view'] = (None)\n",
    "            dict_total['source'] = \"scraped\"\n",
    "            #  \"ايجار\",\"بيع\"\n",
    "            dict_total['rent_sale'] = (category_Dict[cat_num][0])\n",
    "            # \"تجاري\",\"سكني\"\n",
    "            dict_total['type'] = (category_Dict[cat_num][1])\n",
    "            list_dict.append(dict_total.copy())\n",
    "            counter += 1\n",
    "            if (counter == thousand*1000) :\n",
    "            #   print(counter)\n",
    "              thousand+=1\n",
    "    # print(counter)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame.from_dict(list_dict)\n",
    "     #  df.to_csv('propertyFinder_Scraped_data.csv', index=False, header=True, encoding='utf-8-sig')\n",
    "    \n",
    "    # print(\"Scraped\")\n",
    "    \n",
    "    \n",
    "    '''***************************cleaning*****************************************'''\n",
    "    \n",
    "    df['cleaned_bathrooms'] = df['bathrooms'].str.replace(\"+\", \"\")\n",
    "    df.loc[df['cleaned_bathrooms'] == \"غيرمتوفر\", 'cleaned_bathrooms'] = np.nan\n",
    "    df.loc[df['cleaned_bathrooms'] == \"لايوجد\", 'cleaned_bathrooms'] = np.nan\n",
    "    \n",
    "    df = df.astype({'cleaned_bathrooms': float})\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if(df.at[i, 'bedrooms'] == None):\n",
    "            continue\n",
    "        num = df.at[i, 'bedrooms'].split()\n",
    "        if (num[0].replace(\"+\", \"\").isdigit() == True):\n",
    "            if (len(num) > 1):\n",
    "                df.at[i, 'cleaned_bedrooms'] = float(num[0].replace(\"+\", \"\")) + 1\n",
    "            else:\n",
    "                df.at[i, 'cleaned_bedrooms'] = float(num[0].replace(\"+\", \"\"))\n",
    "        else:\n",
    "            string = df.at[i, 'bedrooms'].replace(\" \", \"\")\n",
    "            if ((string == \"غيرمتوفر\") or (string == \"غيرمتوفر+غرفةخادمة\") or (string == \"لايوجد\")):\n",
    "                df.at[i, 'cleaned_bedrooms'] = np.nan\n",
    "            else:\n",
    "                if (string == \"استوديو\"):\n",
    "                    df.at[i, 'cleaned_bedrooms'] = 1.0\n",
    "                else:\n",
    "                 # استوديو + غرفة خادمة\n",
    "                    df.at[i, 'cleaned_bedrooms'] = 2.0\n",
    "    \n",
    "    df['web_name']=df['web_name'].replace('Property finder','بروبرتي فايندر')\n",
    "    df['bedrooms'] = df['cleaned_bedrooms']\n",
    "    df['bathrooms'] = df['cleaned_bathrooms']\n",
    "    \n",
    "    df.drop(['cleaned_bedrooms', 'cleaned_bathrooms'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    df['lon'] = np.nan\n",
    "    df['lat'] = np.nan\n",
    "    \n",
    "    df = df.reindex(columns=['link', 'bathrooms', 'bedrooms', 'city', 'location1', 'location2', 'price',\n",
    "                    'size', 'price_per_unit', 'type', 'unit','lat','lon' ,'view', 'description', 'web_name', 'source', 'rent_sale'])\n",
    "    \n",
    "    \n",
    "    df.to_csv('propertyFinder_cleaned_data.csv', index=False, header=True, encoding='utf-8-sig')\n",
    "        \n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    print(\"end \",end)\n",
    "    \n",
    "    print(\"duration \",end - start)\n",
    "\n",
    "propertyfinder_Scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65bb9cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flats-for-rent\n",
      "page  1\n",
      "block : 1\n",
      "block : 2\n",
      "block : 3\n",
      "block : 4\n",
      "block : 5\n",
      "block : 6\n",
      "block : 7\n",
      "block : 8\n",
      "block : 9\n",
      "block : 10\n",
      "block : 11\n",
      "block : 12\n",
      "block : 13\n",
      "block : 14\n",
      "block : 15\n",
      "block : 16\n",
      "block : 17\n",
      "block : 18\n",
      "block : 19\n",
      "block : 20\n",
      "Done flats-for-rent\n",
      "shkk-k-non-kdym\n",
      "page  1\n",
      "block : 1\n",
      "block : 2\n",
      "block : 3\n",
      "block : 4\n",
      "block : 5\n",
      "block : 6\n",
      "block : 7\n",
      "block : 8\n",
      "block : 9\n",
      "block : 10\n",
      "block : 11\n",
      "block : 12\n",
      "block : 13\n",
      "block : 14\n",
      "block : 15\n",
      "block : 16\n",
      "block : 17\n",
      "block : 18\n",
      "block : 19\n",
      "block : 20\n",
      "Done shkk-k-non-kdym\n",
      "furnished-flats\n",
      "page  1\n",
      "block : 1\n",
      "block : 2\n",
      "block : 3\n",
      "block : 4\n",
      "block : 5\n",
      "block : 6\n",
      "block : 7\n",
      "block : 8\n",
      "block : 9\n",
      "block : 10\n",
      "block : 11\n",
      "block : 12\n",
      "block : 13\n",
      "block : 14\n",
      "block : 15\n",
      "block : 16\n",
      "block : 17\n",
      "block : 18\n",
      "block : 19\n",
      "block : 20\n",
      "Done furnished-flats\n",
      "rooms-for-rent\n",
      "page  1\n",
      "block : 1\n",
      "block : 2\n",
      "block : 3\n",
      "block : 4\n",
      "block : 5\n",
      "block : 6\n",
      "block : 7\n",
      "block : 8\n",
      "block : 9\n",
      "block : 10\n",
      "block : 11\n",
      "block : 12\n",
      "block : 13\n",
      "block : 14\n",
      "block : 15\n",
      "block : 16\n",
      "block : 17\n",
      "block : 18\n",
      "block : 19\n",
      "block : 20\n",
      "Done rooms-for-rent\n",
      "villas-for-rent\n",
      "page  1\n",
      "block : 1\n",
      "block : 2\n",
      "block : 3\n",
      "block : 4\n",
      "block : 5\n",
      "block : 6\n",
      "block : 7\n",
      "block : 8\n",
      "block : 9\n",
      "block : 10\n",
      "block : 11\n",
      "block : 12\n",
      "Done villas-for-rent\n",
      "business-rent-4\n",
      "page  1\n",
      "block : 1\n",
      "block : 2\n",
      "block : 3\n",
      "block : 4\n",
      "block : 5\n",
      "block : 6\n",
      "block : 7\n",
      "block : 8\n",
      "block : 9\n",
      "block : 10\n",
      "block : 11\n",
      "block : 12\n",
      "block : 13\n",
      "block : 14\n",
      "block : 15\n",
      "block : 16\n",
      "block : 17\n",
      "block : 18\n",
      "block : 19\n",
      "block : 20\n",
      "Done business-rent-4\n",
      "shops-for-rent\n",
      "page  1\n",
      "block : 1\n",
      "block : 2\n",
      "block : 3\n",
      "block : 4\n",
      "block : 5\n",
      "block : 6\n",
      "block : 7\n",
      "block : 8\n",
      "block : 9\n",
      "block : 10\n",
      "block : 11\n",
      "block : 12\n",
      "block : 13\n",
      "block : 14\n",
      "block : 15\n",
      "block : 16\n",
      "block : 17\n",
      "block : 18\n",
      "block : 19\n",
      "block : 20\n",
      "Done shops-for-rent\n",
      "offices-rent\n",
      "page  1\n",
      "block : 1\n",
      "block : 2\n",
      "block : 3\n",
      "block : 4\n",
      "block : 5\n",
      "block : 6\n",
      "block : 7\n",
      "block : 8\n",
      "block : 9\n",
      "block : 10\n",
      "block : 11\n",
      "block : 12\n",
      "block : 13\n",
      "block : 14\n",
      "block : 15\n",
      "block : 16\n",
      "block : 17\n",
      "block : 18\n",
      "block : 19\n",
      "block : 20\n",
      "Done offices-rent\n",
      "store-land-rent\n",
      "page  1\n",
      "block : 1\n",
      "block : 2\n",
      "block : 3\n",
      "block : 4\n",
      "block : 5\n",
      "block : 6\n",
      "block : 7\n",
      "block : 8\n",
      "block : 9\n",
      "block : 10\n",
      "block : 11\n",
      "block : 12\n",
      "block : 13\n",
      "block : 14\n",
      "block : 15\n",
      "block : 16\n",
      "block : 17\n",
      "block : 18\n",
      "block : 19\n",
      "block : 20\n",
      "Done store-land-rent\n",
      "equipment-rent\n",
      "page  1\n",
      "block : 1\n",
      "block : 2\n",
      "block : 3\n",
      "block : 4\n",
      "block : 5\n",
      "No Table\n",
      "block : 6\n",
      "block : 7\n",
      "block : 8\n",
      "block : 9\n",
      "Done equipment-rent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-2ef829eac283>:127: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['bathrooms'] = df['bathrooms'].str.replace('+','')\n",
      "<ipython-input-4-2ef829eac283>:128: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['bedrooms'] = df['bedrooms'].str.replace('+','')\n"
     ]
    }
   ],
   "source": [
    "def TageerScrape():\n",
    "    list_of_dict = []\n",
    "    \n",
    "    def scrape():  \n",
    "        for j in kind:\n",
    "#             print(j)\n",
    "            pageNum = 1\n",
    "            url_site = f\"https://tageer.net/bhth.html?title=&category%5b0%5d={j}&location%5b0%5d=cairo&page={pageNum}\"\n",
    "            res_out = requests.get(url_site)\n",
    "            src_out = res_out.content\n",
    "            soup_out = BeautifulSoup(src_out, 'lxml')\n",
    "            try:\n",
    "                lastpage = soup_out.find('a', {'title': 'الأخير '}).attrs['href']\n",
    "            except:\n",
    "                lastpage = url_site\n",
    "            \n",
    "            while True: \n",
    "#                 print(\"page \", pageNum)\n",
    "                url_site = f\"https://tageer.net/bhth.html?title=&category%5b0%5d={j}&location%5b0%5d=cairo&page={pageNum}\"\n",
    "                pageNum += 1\n",
    "                res = requests.get(url_site)\n",
    "                src = res.content\n",
    "                soup = BeautifulSoup(src, 'lxml')\n",
    "                blocks = soup.find_all('a', {'class' : \"big-txt\"})\n",
    "                x = 1\n",
    "                for i in blocks:\n",
    "#                     print(\"block :\",x)\n",
    "                    x += 1\n",
    "                    dic = {}\n",
    "                    url_in = i.attrs['href']\n",
    "                    res_in = requests.get(url_in)\n",
    "                    src_in = res_in.content \n",
    "                    soup_in = BeautifulSoup(src_in, 'lxml')\n",
    "                    ti = i.text.strip()\n",
    "                    try :\n",
    "                        pri = soup_in.find('div', {'class': 'col-xs-12 col-sm-2 price price-curry'}).text.strip()\n",
    "                    except:\n",
    "                        pri = \"\"\n",
    "                    try:\n",
    "                        loc = soup_in.find('div', {'class': 'col-xs-12 col-sm-2'}).text.strip()\n",
    "                    except:\n",
    "                        loc = \"\"\n",
    "                    try:\n",
    "                        dis = soup_in.find('div', {'class': 'tab-pane fade active in'}).find('p').text.strip()\n",
    "                    except:\n",
    "                        dis = \"\"\n",
    "                    try:\n",
    "                        all_de = soup_in.find('table', {'class': 'table table-striped'}).find_all('tr')\n",
    "                        for i in all_de:\n",
    "                            key = i.find('td').text.strip()\n",
    "                            value = i.find('td').findNext('td').text\n",
    "                            dic[key] = value \n",
    "                    except:\n",
    "                        print(\"No Table\")\n",
    "\n",
    "                    dic['link'] = url_in\n",
    "                    dic['title'] = ti\n",
    "                    dic['price'] = pri\n",
    "                    dic['location1'] = loc\n",
    "                    dic['description'] = dis\n",
    "                    dic['type'] = j\n",
    "                    list_of_dict.append(dic)\n",
    "#                 break\n",
    "                if url_site == lastpage:\n",
    "                    break\n",
    "            \n",
    "#             print(\"Done\",j)\n",
    "\n",
    "    kind = ['flats-for-rent', 'shkk-k-non-kdym', 'furnished-flats', 'rooms-for-rent', 'villas-for-rent', 'business-rent-4',\n",
    "             'shops-for-rent', 'offices-rent', 'store-land-rent', 'equipment-rent']\n",
    "    \n",
    "    scrape()\n",
    "    print(\"Done Tageer\")\n",
    "    df = pd.DataFrame(list_of_dict)\n",
    "    #df.to_csv(r'Tageer.csv', index = False, header = True)\n",
    "\n",
    "\n",
    "    df.rename(columns= {'عدد الحمامات' : 'bathrooms',\n",
    "                        'عدد الغرف' : 'bedrooms', \n",
    "                        'المساحة بالمتر' : 'size'\n",
    "                       }, inplace = True,errors='raise')\n",
    "\n",
    "    df.drop(['مفروش', 'الطابق', 'يضاف رسوم السمسرة','الأسانسير','السعر قابل للتفاوض', 'قيمة المقدم'], axis = 1, inplace = True)\n",
    "    df.dropna(inplace = True)\n",
    "\n",
    "    df['price'] = df['price'].str.replace('EGP',\"\")\n",
    "    df['price'] = df['price'].str.replace(',',\"\")\n",
    "    df.drop(df.index[df['price'] == ''], inplace = True)\n",
    "    df.drop(df.index[df['size'] == ''], inplace = True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['price'] = df['price'].astype(float)\n",
    "    df['size'] = df['size'].astype(float)\n",
    "    del df['index']\n",
    "    \n",
    "    ####\n",
    "    df['city'] = \"القاهرة\"\n",
    "    df['location2'] = df['location1']\n",
    "    df['price_per_unit'] = df['price'] / df['size']\n",
    "    df['web_name'] = 'تأجير' \n",
    "    df['view'] = None\n",
    "    df['source'] = 'scraped'\n",
    "    df['rent_sale'] = 'ايجار'\n",
    "    df['lat'] = None\n",
    "    df['lon'] = None\n",
    "    ###\n",
    "    typeMaping = {'flats-for-rent' : 'شقق',\n",
    "          'shkk-k-non-kdym' : 'شقق 59سنة',\n",
    "          'furnished-flats' : 'شقق مفروشة',\n",
    "          'rooms-for-rent' : 'غرف',\n",
    "          'villas-for-rent' : 'فيلات و شاليهات',\n",
    "          'business-rent-4' : 'مشروعات',\n",
    "          'shops-for-rent' : 'محلات',\n",
    "          'offices-rent' : 'مكاتب ومباني',\n",
    "          'store-land-rent' : 'مخازن واراضي',\n",
    "          'equipment-rent' : 'مصانع ومعدات'}\n",
    "    newTyping = {\n",
    "          'شقق' : 'سكني',\n",
    "          'شقق 59سنة' : 'سكني',\n",
    "          'شقق مفروشة' : 'سكني',\n",
    "          'غرف' : 'سكني',\n",
    "          'فيلات و شاليهات' : 'سكني',\n",
    "          'مشروعات' : 'تجاري',\n",
    "          'محلات' : 'تجاري',\n",
    "          'مكاتب ومباني' : 'تجاري',\n",
    "          'مخازن واراضي' : 'تجاري',\n",
    "          'مصانع ومعدات' : 'تجاري'\n",
    "    }\n",
    "    df['bathrooms'] = df['bathrooms'].str.replace('+','')\n",
    "    df['bedrooms'] = df['bedrooms'].str.replace('+','')\n",
    "    df['bathrooms'] = pd.to_numeric(df.bathrooms.apply(unidecode), errors='coerce')\n",
    "    df['bedrooms'] = pd.to_numeric(df.bedrooms.apply(unidecode), errors='coerce')\n",
    "    \n",
    "    df['bedrooms'] = df['bedrooms'].astype(int)\n",
    "    df.drop(df[df['bedrooms'].isna()].index , inplace =True)\n",
    "    df.drop(df[df['description'].isna()].index , inplace =True)\n",
    "    \n",
    "    \n",
    "    df[\"unit\"] = df['type'].map(typeMaping)\n",
    "    df.drop(['type'], axis = 1, inplace = True)\n",
    "    df['type'] = df['unit'].map(newTyping)\n",
    "    #print(df.info())\n",
    "    df.drop_duplicates()\n",
    "    df = df.reindex(columns=['link',  'bathrooms',  'bedrooms', \n",
    "    'city', 'location1', 'location2', 'price', 'size', 'price_per_unit', 'type',  'unit', \n",
    "    'lat', 'lon', 'view', 'description', 'web_name', 'source', 'rent_sale'])\n",
    "    df.to_csv(r'Tageer1.csv', index = False, header = True, encoding ='utf-8-sig')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TageerScrape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a8938b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-533c6143dfe4>:93: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['bathrooms'] = df['bathrooms'].str.replace('+', '')\n"
     ]
    }
   ],
   "source": [
    "def open_souqscrapping():\n",
    "\n",
    "  def next_page(headers, soup):\n",
    "    next_page = soup.find('li', {'class':'next'}).find('span')['onclick'].replace(\"location.href='\" , '').replace(\"'\" , '')\n",
    "    result = requests.get('https://eg.opensooq.com' + next_page)\n",
    "    src=result.content\n",
    "    soup = BeautifulSoup(src,\"lxml\")\n",
    "    return soup\n",
    "\n",
    "  def get_all_links(url):\n",
    "  all_links = []\n",
    "  headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',}\n",
    "  result = requests.get(url, headers=headers)\n",
    "  soup = BeautifulSoup(result.content, 'lxml')\n",
    "  while soup:      \n",
    "    links_in_page = soup.find_all('h2' , {'class':'fRight mb15'})\n",
    "    links = ['https://eg.opensooq.com' + link.find('span')['data-href'] for link in links_in_page]\n",
    "    # print(links)\n",
    "    all_links.extend(links)\n",
    "    try:\n",
    "      soup = next_page(headers, soup)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      break\n",
    "\n",
    "  return all_links\n",
    "\n",
    "  global df_list \n",
    "  df_list = []\n",
    "  def scrap_data(loction, url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',}\n",
    "    links_list = get_all_links(url)\n",
    "    # print(links_list)\n",
    "    for link in links_list:\n",
    "      property_dict = {}\n",
    "      try:\n",
    "        result = requests.get(link, headers)\n",
    "        soup = BeautifulSoup(result.content, 'lxml')\n",
    "        table = soup.find_all('li', {'class':'inline vTop relative mb15'})\n",
    "        for i in table:\n",
    "          try:\n",
    "            td1 = i.text.replace(i.find_next('span').text, '').strip()\n",
    "          except:\n",
    "            td1 = None\n",
    "          try:\n",
    "            td2 = i.find_next('span').text.strip()\n",
    "          except:\n",
    "            td2 = None\n",
    "          try:\n",
    "            property_dict[td1] = td2\n",
    "          except:\n",
    "            property_dict[td1] = None\n",
    "        try:\n",
    "          property_dict['link'] = link.strip()\n",
    "        except:\n",
    "          property_dict['link'] = None\n",
    "        row = soup.find('ul',{'class':'breadcrumbs'}).find_all('li')\n",
    "        try:\n",
    "          property_dict['rent_sale'] = row[2].text\n",
    "        except:\n",
    "          property_dict['rent_sale'] = None\n",
    "        try:\n",
    "          property_dict['unit'] = row[3].text\n",
    "        except:\n",
    "          property_dict['unit'] = None\n",
    "        try:\n",
    "          property_dict['description'] = soup.find('div', {'class':'postDesc overflowHidden'}).find('p').text.strip().replace(\"\\t\",\"\").replace(\"\\r\",\" \").replace(\"\\n\",\"\")\n",
    "        except:\n",
    "          property_dict['description'] = None\n",
    "        try:\n",
    "          property_dict['web_name'] = 'السوق المفتوح'\n",
    "        except:\n",
    "          property_dict['web_name'] = None\n",
    "        property_dict_copy= property_dict.copy()\n",
    "        df_list.append(property_dict_copy)\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "  location = 'القاهرة'\n",
    "  rent_or_sale =['عقارات-للبيع', 'عقارات-للايجار']\n",
    "  for rent in rent_or_sale:\n",
    "      url = f'https://eg.opensooq.com/ar/{location}/{rent}/all'\n",
    "      scrap_data(location, url)\n",
    "\n",
    "  df = pd.DataFrame.from_dict(df_list)\n",
    "\n",
    "  # df.to_csv('cairo.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "  ##--------------------------------------------------------------------- CLEAN ----------------------------------------------------------------------------------------------##\n",
    "\n",
    "  df = df.rename(columns={'المدينة :':'city', 'الحي:':'location1', 'عدد الغرف:':'bedrooms', 'عدد الحمامات:':'bathrooms', 'مساحة البناء:':'size','السعر:':'price', 'النوع:':'type'})\n",
    "  # df.drop(['الطابق:', 'مفروشة/غير مفروشة:', 'عمر البناء:', 'مساحة الأرض:', 'عدد الطوابق:', 'مخصصة لـ:','مدة الإيجار:',  'طريقة الدفع:', 'الدولة:', 'نوع العقار:' ], axis=1, inplace=True)\n",
    "  df.drop(['الطابق:', 'مفروشة/غير مفروشة:', 'عمر البناء:', 'مساحة الأرض:', 'عدد الطوابق:', 'مخصصة لـ:','مدة الإيجار:',  'طريقة الدفع:' ], axis=1, inplace=True)\n",
    "\n",
    "  df['price'] = df['price'].str.replace('جنيه', '') \n",
    "  df['price'] = df['price'].str.replace(',', '')\n",
    "  df['bathrooms'] = df['bathrooms'].str.replace('حمّامين', '٢') \n",
    "  df['bathrooms'] = df['bathrooms'].str.replace('حمّامات', '') \n",
    "  df['bathrooms'] = df['bathrooms'].str.replace('حمّام', '١') \n",
    "  df['bathrooms'] = df['bathrooms'].str.replace('+', '')\n",
    "  df['bedrooms'] = df['bedrooms'].str.replace('غرف نوم', '')\n",
    "  df['bedrooms'] = df['bedrooms'].str.replace('غرفة نوم', '1')\n",
    "  df['bedrooms'] = df['bedrooms'].str.replace('غرفتا نوم', '')  \n",
    "  df['bedrooms'] = df['bedrooms'].str.replace('غرف', '')\n",
    "  df['bedrooms'] = df['bedrooms'].str.replace('ستوديو', '1')\n",
    "  df['bedrooms']=df['bedrooms'].replace('١٠+','10')\n",
    "  df['bathrooms']=df['bathrooms'].replace('١٠+','10')\n",
    "  df[\"bathrooms\"] = df[\"bathrooms\"].fillna('0').astype(int)\n",
    "  df[\"bedrooms\"] = df[\"bedrooms\"].fillna('0').astype(int)\n",
    "  df[\"bedrooms\"] = df[\"bedrooms\"].replace(0,np.nan)\n",
    "  df[\"bathrooms\"] = df[\"bathrooms\"].replace(0,np.nan)\n",
    "  df['size'] = df['size'].str.replace('م٢', '')\n",
    "  df['unit'] = df['unit'].str.replace('للبيع', '') \n",
    "  df['unit'] = df['unit'].str.replace('للايجار', '')\n",
    "  df['rent_sale'] = df['rent_sale'].str.replace('للبيع', 'بيع') \n",
    "  df['rent_sale'] = df['rent_sale'].str.replace('للايجار', 'إيجار')\n",
    "  df['unit'] = df['unit'].str.replace('\\n', '')\n",
    "  df['rent_sale'] = df['rent_sale'].str.replace('عقارات', '')\n",
    "\n",
    "  df['location2'] = df['location1']\n",
    "\n",
    "  df['size'] = pd.to_numeric(df['size'], errors='coerce') \n",
    "  df['price'] = pd.to_numeric(df['price'], errors='coerce') \n",
    "  df['price_per_unit'] = df['price'] / df['size']\n",
    "  df['price_per_unit'] = df['price_per_unit'].round(2)\n",
    "\n",
    "  column_names = ['link',     \n",
    "                  'bathrooms',\n",
    "                  'bedrooms',\n",
    "                  'city',\n",
    "                  'location1',\n",
    "                  'location2',\n",
    "                  'price',\n",
    "                  'size',                     \n",
    "                  'price_per_unit',\n",
    "                  'type',\n",
    "                  'unit',\n",
    "                  'lat',\n",
    "                  'lon',\n",
    "                  'view',\n",
    "                  'description',\n",
    "                  'web_name',\n",
    "                  'source', \n",
    "                  'rent_sale']\n",
    "  df = df.reindex(columns=column_names)\n",
    "\n",
    "  df['type'] = df['type'].str.replace('مكاتب - عيادات', 'إدارى')\n",
    "  df['type'] = df['type'].str.replace('محل', 'تجارى') \n",
    "  df['type'] = df['type'].str.replace('مخزن/مستودع', 'تجارى') \n",
    "  df['type'] = df['type'].str.replace('مجمّع/مبنى تجاري', 'تجارى')\n",
    "  df['type'] = df['type'].str.replace('معرض', 'تجارى') \n",
    "  df['type'] = df['type'].str.replace('مطعم - كوفي شوب', 'تجارى')\n",
    "  df['type'] = df['type'].str.replace('فندق', 'سكنى') \n",
    "  df['type'] = df['type'].str.replace('طابق كامل', 'إدارى') \n",
    "  df['type'] = df['type'].str.replace('سوبر ماركت', 'تجارى') \n",
    "  df['type'] = df['type'].str.replace('سكن موظفين', 'سكنى') \n",
    "  df['type'] = df['type'].str.replace('فيلا تجارية', 'تجارى')\n",
    "\n",
    "  df['type'].fillna('سكنى',inplace=True)\n",
    "\n",
    "  df= df.dropna( how='any', subset=['price'])  \n",
    "  df= df.dropna( how='any', subset=['size']) \n",
    "  df= df.dropna( how='any', subset=['location1'])\n",
    "  df['source'] = 'Scraped'\n",
    "  df['web_name'] = 'السوق المفتوح'\n",
    "  df.drop(df[df['type'] == 'أخرى'].index, inplace = True)\n",
    "  df.drop(df[df['location1'] == 'أخرى'].index, inplace = True)\n",
    "\n",
    "  df = pd.DataFrame.from_dict(df, orient='columns')\n",
    "  df.to_csv('opensooq.csv', index=False,  encoding ='utf-8-sig')\n",
    "\n",
    "\n",
    "open_souqscrapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d79d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semsar_function():\n",
    "    \n",
    "    gov_id = 984\n",
    "    \n",
    "    page_num = 1\n",
    "    links = []\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    unites = []\n",
    "    locations = []\n",
    "    dictionaries_list = []\n",
    "    url = []\n",
    "    while True :\n",
    "        try:\n",
    "            result = requests.get(f\"https://www.semsarmasr.com/3akarat?title=%C7%DA%E1%C7%E4%content20%DA%DE%C7%D1%C7%CA%20%DD%ED%20%C7%E1%DE%C7%E5%D1%C9%20%E3%D5%D1&r=70&g=984&a=0&cid=0&p={page_num}\")\n",
    "            soup = BeautifulSoup(result.content , \"lxml\" )\n",
    "\n",
    "            page = soup.find('div',{'id':'paging'}).find_all(\"a\"  ) #,{\"id\" : \"PageNum\"}\n",
    "\n",
    "            for i in range(len(page)):\n",
    "                url.append(page[i].text)\n",
    "                \n",
    "        #    print(url)\n",
    "       \n",
    "        except:\n",
    "            url.append('null')\n",
    "        \n",
    "        \n",
    "        \n",
    "        page_limit = url[-3]\n",
    "    \n",
    "        divs = soup.find_all('div' , {'class':'ListInfo'})\n",
    "        #print(len(divs))\n",
    "        for div in divs:\n",
    "            link = div.find('a')['href']\n",
    "            links.append(link)\n",
    "    \n",
    "        #print(page_num , page_limit)\n",
    "    \n",
    "        if page_num == int(page_limit):\n",
    "            break\n",
    "    \n",
    "        page_num+=1\n",
    "\n",
    "                           \n",
    "    for link in links:\n",
    "        result = requests.get(link)\n",
    "        src = result.content\n",
    "        soup = BeautifulSoup(src ,\"lxml\")\n",
    "        table = soup.find_all('div' , {'id':'DetRrow'})\n",
    "        try:\n",
    "            title = soup.find(\"div\" , {\"class\" : \"PageTitlesStyle\"})\n",
    "        except: title='null'\n",
    "\n",
    "        try:\n",
    "            description = soup.find(\"p\" , {\"style\" : \"padding:5px;white-space:pre-line\"})\n",
    "        except: description = 'null'\n",
    "\n",
    "        try:\n",
    "            city = soup.find('div' , {'class':'PathStyle'}).find_all(\"span\"  )[2]\n",
    "        except: city = 'null'\n",
    "\n",
    "        try:\n",
    "            location1 = soup.find('div' , {'class':'PathStyle'}).find_all(\"span\"  )[3]\n",
    "        except: location1 = 'null'\n",
    "\n",
    "        try:\n",
    "            location2 = soup.find('div' , {'class':'PathStyle'}).find_all(\"span\"  )[4]\n",
    "        except: location2 = 'null'\n",
    "\n",
    "        try:\n",
    "            unit = soup.find('div' , {'class':'PathStyle'}).find_all(\"span\"  )[-1]\n",
    "        except: unit = 'null'\n",
    "\n",
    "        #print(\"loc >>>\" location.text)\n",
    "        #print(\"title >>> \"title.text)\n",
    "\n",
    "        # print(titles)\n",
    "        # print(descriptions)\n",
    "        # print(locations)\n",
    "        dic = {}\n",
    "        for i in table:\n",
    "            key = i.find('div')\n",
    "            value = key.find_next('div')\n",
    "            dic[key.text.strip()] = value.text.strip()\n",
    "\n",
    "            dic[\"source\"] = \"scraped\"\n",
    "            dic[\"web_name\"] = \"سمسار\"\n",
    "            dic[\"lon\"] = \"None\"\n",
    "            dic[\"lat\"] = \"None\"\n",
    "\n",
    "        try:\n",
    "            dic[\"title\"] = title.text.strip()\n",
    "        except: dic[\"title\"] =  'null'\n",
    "\n",
    "        try:\n",
    "            dic['description'] = description.text.strip()\n",
    "        except: dic['description'] =  'null'\n",
    "\n",
    "        try:\n",
    "            dic[\"location1\"] = location1.text.strip()\n",
    "        except: dic[\"location1\"] =  'null'\n",
    "\n",
    "        try:\n",
    "            dic[\"city\"] = city.text.strip()\n",
    "        except: dic[\"city\"] =  'null'\n",
    "\n",
    "        try:\n",
    "            dic[\"location2\"] = location2.text.strip()\n",
    "        except:  dic[\"location2\"] =  'null'\n",
    "\n",
    "        try:\n",
    "            dic[\"unit\"] = unit.text.strip()\n",
    "        except:  dic[\"unit\"] =  'null'\n",
    "\n",
    "        try:\n",
    "            dic[\"link\"] = link\n",
    "        except:  dic[\"link\"] = 'null'\n",
    "\n",
    "        #pprint(dic)\n",
    "\n",
    "        #print(key.text, ' >>>>>>>> ' , value.text)\n",
    "        #print(dic)\n",
    "        dictionaries_list.append(dic) \n",
    "\n",
    "    df = pd.DataFrame (dictionaries_list)\n",
    "\n",
    "\n",
    "# Cleaning the data to put it in determined format\n",
    "\n",
    "\n",
    "    \n",
    "    #df.head(n=10)\n",
    "    df.rename({'السعر:': 'price', 'الغرض:':'rent_sale' , 'القسم:':'type' , 'المساحة:':'size' , 'عدد الغرف:':'bedrooms' , 'عدد الحمامات:':'bathrooms' }, axis=1, inplace=True)\n",
    "    #df.head(n=2)\n",
    "    df['view'] = ''\n",
    "    df.head(n=2)\n",
    "    #print(df.columns)\n",
    "    df.drop(['رقم الدور:' , 'title' , 'نوع التشطيب:' ], axis=1, inplace=True)\n",
    "    df.drop(['الفرش:' , 'رقم الإعلان:' ], axis=1, inplace=True)\n",
    "    #print(df.columns)\n",
    "    df.drop(['تاريخ البناء:' , 'عدد الأدوار:' , \"طبيعة المُعلن:\" ], axis=1, inplace=True)\n",
    "    #df.head(n=5)\n",
    "    df['price']=df['price'].str.replace(',','')\n",
    "    #df.head(n=5)\n",
    "    df['price'] = df.price.str.extract('(\\d+)')\n",
    "    df['price']\n",
    "    df['unit']=df['unit'].str.extract('(^[^\\s]+)')\n",
    "    df['unit']\n",
    "    #df.head(n=5)\n",
    "    df['size']=df['size'].str.replace(',','')\n",
    "    df = df.assign(size = lambda x: x['size'].str.extract('(\\d+)'))\n",
    "    df['size']\n",
    "    #df.head(n=5)\n",
    "    df['price'] = df['price'].astype(str)\n",
    "    df['price'] = df['price'].astype(float)\n",
    "    df['size'] = df['size'].astype(float)\n",
    "    df[\"price_per_unit\"] = df[\"price\"].div(df[\"size\"].values)\n",
    "    df[\"location2\"] = df[\"location1\"]\n",
    "\n",
    "    df.loc[df['type'] == 'عقارات سكنية', 'type'] = 'سكني'\n",
    "    df.loc[df['type'] == 'أراضي زراعية ومزارع للبيع', 'type'] = 'اراضي'\n",
    "    df.loc[df['type'] == 'عقارات تجارية', 'type'] = 'تجاري'\n",
    "    df.loc[df['type'] == 'عقارات إدارية', 'type'] = 'اداري'\n",
    "    df.loc[df['type'] == 'عقارات سياحية', 'type'] = 'تجاري'\n",
    "    df.loc[df['type'] == 'عقارت طبية', 'type'] = 'تجاري'\n",
    "    df.loc[df['type'] == 'عقارات صناعية', 'type'] = 'تجاري'\n",
    "    df.loc[df['type'] == 'عقارات متنوعة', 'type'] = 'تجاري'\n",
    "    df.loc[df['type'] == 'عقارات تعليمية', 'type'] = 'تجاري'\n",
    "    df['rent_sale'] = df['rent_sale'].str.replace('للبيع', 'بيع') \n",
    "    df['rent_sale'] = df['rent_sale'].str.replace('للايجار', 'إيجار')\n",
    "    #df.head(n=5)\n",
    "    #print(df.columns)\n",
    "    df = df.reindex(columns=['link', 'bathrooms', 'bedrooms', 'city', 'location1', 'location2', 'price',\n",
    "                             'size', 'price_per_unit', 'type', 'unit','lat','lon' ,'view', 'description', 'web_name', 'source', 'rent_sale'])\n",
    "    df.head(n=5)\n",
    "    df.to_csv('semsar_data.csv', index=False, header=True, encoding='utf-8-sig')\n",
    "\n",
    "# Calling the Function\n",
    "\n",
    "semsar_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2deac8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_csv('Tageer1.csv')\n",
    "df2= pd.read_csv('semsar_data.csv')\n",
    "df3= pd.read_csv('olx_Allcairo_clean.csv')\n",
    "df4= pd.read_csv('opensooq.csv')\n",
    "df5= pd.read_csv('propertyFinder_cleaned_data.csv')\n",
    "#concat\n",
    "df = pd.concat([df1, df2,  df3, df4, df5])\n",
    "\n",
    "df.dropna(subset = [\"location1\"], inplace=True)\n",
    "df.dropna(subset = [\"location2\"], inplace=True)\n",
    "df.dropna(subset = [\"price\"], inplace=True)\n",
    "df.dropna(subset = [\"size\"], inplace=True)\n",
    "df.drop(df[df['location1'] == 'عقارات سكنية في القاهرة مصر'].index,inplace = True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "df['location1']=df['location1'].str.split('،').str[0]\n",
    "df['location1']=df['location1'].str.split(',').str[0]\n",
    "df['location2']=df['location2'].str.split('،').str[0]\n",
    "df['location2']=df['location2'].str.split(',').str[0]\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('myAllData.csv', index=False, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379b006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ad884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
